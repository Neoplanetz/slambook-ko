![carrot](/resources/Cover.png)

---

# Preface

이 문서는 중국어 원서인 “입문 Visual SLAM 이론에서 연습까지 14 강(视觉SLAM十四讲 从理论到实践)” 책의  원저자로부터 한글 번역 허가를 받고 구글 번역기를 이용하여 작성된 문서입니다. 본 문서는 아래의 Contribution을 특징으로 합니다.

- 중국어 전공 서적을 구글 번역기를 이용해 한글로 초벌 번역했습니다.
- 초벌 번역 후 매끄럽지 않은 문장은 문맥에 맞게 수정되었습니다.
- 문서 내용 중 참고할만한 웹문서를 코멘트로 추가했습니다.  
- SLAM 연구에서 주로 사용되는 용어는 한글로 번역된 용어보다 주로 사용되는 영어로 된 용어 그대로 표시하였습니다.

그럼에도 불구하고 부정확하거나 매끄럽지 않은 부분이 있을수 있습니다. 그런 부분은 코멘트로 제안해주시면 반영하도록 노력하겠습니다. 또한 읽으시다가 잘 이해가 가지 않는 부분도 코멘트로 질문해주시면 답변해드리도록 하겠습니다.

번역 참가자:
- 신동원 (광주과학기술원 박사과정)
- 김선호 (VIRNECT 선임연구원)
- 조원재 (일본국립농업기술혁신공학센터 연구원)
- 장형기 (Imperial College London 석사)
- 박준영 (광주과학기술원 석사과정)

2018년 10월 1일
신동원 드림

---
# 제2장. SLAM과의 첫 만남.

> **주요 목표**
>  1. 시각적 SLAM 프레임 워크로 구성되는 모듈과 각 모듈의 태스크가 무엇인지 이해하도록 합니다.
>  2. 개발 및 실험을 준비하기위한 프로그래밍 환경을 설정하십시오.
>  3. Linux에서 프로그램을 컴파일하고 실행하는 방법을 이해하고 프로그램에 문제가 있는 경우 디버깅하는 방법을 이해하도록 합니다.
>  4. cmake의 기본 사용을 마스터하십시오.  

이 강의에서는 후속 컨텐츠를 위해 시각적 SLAM 시스템의 구조에 대한 개요 제공합니다. 실용적인 부분에서는 환경 구성과 프로그램에 대한 기본 지식을 소개하고 마지막으로 "Hello SLAM"프로그램을 완성합니다.

# 1.2 서론: 작은 로봇의 예  

그림 2-1과 같이 보이는 "Little Radish"라는 로봇을 조립했다고 가정해 봅시다.

![carrot](/resources/ch2/carrot.jpg)    
그림 2-1 작은 로봇 디자인. 왼쪽 : 정면도, 오른쪽 : 측면도. 카메라, 바퀴, 수첩, 손잡이가있는 장비

이 로봇은 "Android"와 조금 비슷하게 생겼습니다. 이것은 무엇을 할 수 있을까요? 우리는 이 작은 로봇이 자율적으로 움직이기를 원합니다. 하나의 기계로서 독자들은 로봇이 방안에서 자유롭게 움직일 수 있기를 바랍니다.  
첫째로 로봇이 움직이기 위해서는 바퀴와 모터가 있어야합니다. 그래서 우리는 이 작은 로봇 아래에 바퀴를 설치했습니다. 단순히 바퀴를 사용한다고 하면 로봇은 움직일 수는 있겠지만 제어가 불가능합니다. 이 로봇은 목표없이 단지 주변을 돌아 다닐 수 있으며, 나쁜 경우 벽에 부딪쳐 손상을 입을 수 있습니다. 이를 방지하기 위해 카메라를 머리에 설치했습니다.  

카메라 설치를 위한 주요 동기:
로봇은 인간과 매우 유사하다고 생각되어야 합니다 - 그림에서 한눈에 볼 수 있습니다. 때문에, 눈과 두뇌를 가져야 할 것 입니다. 로봇이 방을 탐험하기 위해서는 적어도 다음 두 가지 사항을 알아야합니다.

  1. 나는 어디 있습니까? - 포지셔닝 (Positioning)
  2. 주변 환경은 어떻습니까? - 맵핑 (Mapping)

한편으로는 자신의 위치를 이해하고, 다른 한편으로는 외부 환경 (즉,지도)을 이해해야 합니다. 그러나 이러한 두 가지 문제를 해결할 수있는 방법은 다양합니다. 방의 바닥, 벽면에 가이드 라인을 놓을 수 있다고 가정 해 봅시다. 즉, QR 코드가 벽에 부착되고 무선 위치 확인 장치가 테이블 위에 놓여있다고 말입니다. 휴대 전화나 자동차와 같은 위치 확인 장치를 설치했다고 한다면 이러한 것들로 포지셔닝 문제는 해결되었다고 볼 수 있겠습니다. 앞서 언급한 이 센서들을 두 범주로 나눌 수도 있습니다.(그림 2-2 참조) 로봇의 몸체에는 로봇의 휠 엔코더, 카메라, 레이저 감지기와 같은 센서가 있습니다. 다른 유형은 위에서 언급 한 레일, QR 코드 로고 등과 같이환경에 설치됩니다. 환경 내의 감지 장치는 대개 로봇에 대한 위치 정보를 직접 측정 할 수 있으며 위치 결정 문제를 간단하고 효과적으로 해결할 수 있습니다.

질문: 그렇지만 어떤 장소에서는 실제로 GPS 신호가 없으며, 가이드 레일을 눕힐 수 없는 곳이 있을 수 있습니다. 로봇은 어떻게 위치를 잡을 수 있을까요?  

![sensors](/resources/ch2/sensors.jpg)  
그림 2-2 - 일부 센서. (a) 위치 확인을위한 증강 현실 소프트웨어, (b) GPS 위치 확인 장치, (c)레일이있는 자동차(d) 레이저 레이더, (e) IMU 유닛, (f) 양안 카메라.

우리는 그림 2-2의 센서들이 외부 환경을 제약한다는 것을 알 수 있습니다. 이 특성은 그들의 위치에 따라 만족 될 때에만 프로그램이 작동 할 수 있습니다. 반대로, 제한 조건을 충족 할 수 없는 경우에는 해당 제약 조건을 찾을 수가 없습니다. 결과적으로 이 센서는 간단하고 신뢰할 수 있지만 범용적이고 다양한 솔루션을 제공하지는 못합니다.  
반면에, 레이저 센서, 카메라, 휠 엔코더, 관성 측정 장치 혹은 Measurement Unit, IMU 등에서는 일반적으로 간접적인 물리량을 측정합니다. 예를 들어 휠 엔코더는 휠이 회전하는 각도를 측정하고 IMU는 모션의 각속도와 가속도를 측정합니다. 카메라와 레이저 센서는 외부 환경에서 일부 관측 데이터를 읽습니다. 우리는 간접적인 수단을 통해서만 자신의 위치를 얻을 수 있습니다. 우회적인 방법 같아보이지만, 이런 방법은 어떠한 주어진 환경에 별도의 추가적인 장치없이 위치를 얻을 수 있는 프레임워크를 적용할 수 있다는 장점이 있습니다.  
앞서 논의한 SLAM의 정의를 되돌아 보면, SLAM에서는 알려지지 않은 환경에 대한 문제를 강조하고 있습니다. 특히, 시각적 SLAM에 관해서는 주로 카메라로 위치 문제를 해결하고 이 문제를 해결하는 방법을 사용합니다.
Visual SLAM은 이 책의 주제이기 때문에 로봇의 눈이 되는, 즉 카메라가, 무엇을 할 수 있는지에 대해 고려하는것이 중요합니다.
SLAM카메라에 사용되는 카메라는 일반적으로 보는 SLR 카메라와 달리, 더 단순합니다. 일정한 속도로 주변 환경을 촬영하여 연속적인 비디오 스트림을 형성합니다. 보통의 카메라는 초당 30 프레임으로 이미지를 캡쳐하며 고속 카메라는 더 빠릅니다. 작동하는 방식에 따라서 구분하자면 다른 점은 단안 용 카메라 (Monocular), 양안 용 카메라 (Stereo) 및 깊이 카메라 (RGB-D)로 분류 할 수 있습니다.  
세 가지 카테고리가 그림 2-3에 나와 있습니다. 직관적으로 단안 카메라에는 단 하나의 카메라만 있고 양안 카메라에서는 두 개의 카메라가 있으며 깊이 카메라의 RGB-D 원리는 더 복잡합니다. 이는 컬러 이미지를 캡쳐 할 수 있을뿐만 아니라 각 물체와 카메라 사이의 거리도 읽을 수 있습니다. 깊이 카메라는 일반적으로 여러 대의 카메라를 가지고 있으며 작동 원리는 일반적인 카메라와 다릅니다.  이와 관련된 자세한 내용을 다섯 번째 강의에서 설명하도록 하겠습니다.  
작동 원리를 이해하는데 있어 이 책에서는 독자가 직관적인 개념만 가지고 있으면 됩니다. 또한 SLAM에는 파노라마 및 이벤트 카메라가 있습니다. 이 카메라들은  때때로 SLAM에서 볼 수 있지만 아직 주류가 되지는 않았습니다.  

![camera](/resources/ch2/camera.jpg)  
그림 2-3다양한 종류의 카메라 : 단안, 양안 및 깊이 카메라.  

이제, SLAM을 수행하는 데 사용되는 다양한 카메라의 특성을 살펴 보겠습니다.  

**단안 카메라**  

SLAM에 카메라 하나만 사용하는 것을 Monocular SLAM이라고합니다. 센서 구조는 특히 간단하고 비용은 매우 낮으므로 연구자들은 단안 카메라를 SLAM에 적용하도록 하는 주제에 대해 꽤 고려하고 있습니다. 독자가 단안의 카메라 데이터를 보았다고 생각 해 봅시다. 그 특성은 무엇입니까?  
사진은 본질적으로 카메라의 영상 평면에서 사진을 찍을 때 캡쳐된 장면의 투사입니다. 그것은 2차원에서 3차원 세계를 반영합니다. 분명히 이 투사과정은 전체 3차원 장면에서부터 한 차원을 잃어 버렸습니다.  
단안 카메라에서 우리는 하나의 이미지로부터 장면 내의 물체들 (먼 곳과 가까운 곳) 사이의 거리를 계산할 수 없습니다. 앞서 언급한 하나의 차원(z축)을 잃어버렸기 때문입니다. 나중에 이 거리는 SLAM에서 매우 중요한 정보가 될 것 입니다.  
좀 더 거리에 대해 생각해 봅시다. 이미지는 물체이며 대략적인 크기를 알 수 있습니다. 예를 들어, 근처의 물체는 먼 물체를 가리고 태양과 달과 같은 천체는 일반적으로 멀리 떨어져 있습니다. 물체는 조명을 받고 나서 그림자를 남기게됩니다. 이 정보는 물체의 거리를 판단하는 데 도움이 될 수 있지만 거리감을 무효화하는 몇 가지 상황이 있습니다. 이 시점에서는 물체의 거리와 실제 크기를 판단 할 수 없습니다. 그림 2-4는 이러한 예를 보여줍니다. 이 이미지만으로는 이미지 평면에 투영된 어린 아이들이 진짜 사람인지 작은 사람인지 판단하는 데 어려움이 있습니다. 우리가 원근법을 바꾸지 않는 한, 이 문제를 정확히 해결하기 위해서는 장면의 3차원 구조를 관찰해야 합니다. 즉, 단일 이미지에서 객체의 실제 크기를 결정할 수 없다는 말로 생각할 수 있습니다. 이 아이들은 크지 만 멀리있는 물체 일 수도 있고 아주 가깝지만 작은 물체 일 수도 있습니다.  

![why-depth](/resources/ch2/why-depth.jpg)  
그림 2-4 단안 시각 : 손바닥 위에 있는 물체는 사람일까요, 모델일까요?  

단안 카메라로 찍은 이미지는 3차원 공간의 2차원 투영이므로, 실제로 3차원 구조를 복원하려면 카메라의 화각을 변경해야합니다. 단안 SLAM문제에서도 같은 원리가 적용됩니다. 카메라를 움직여 움직임을 추정하고 장면에서 물체의 거리와 크기를 추정해야 합니다. 그렇다면 이러한 움직임과 물체의 정보를 어떻게 추정할 수 있을까요? 인간의 시각에서는, 선험적인 경험으로부터 우리는 카메라가 오른쪽으로 움직인다면 이미지의 물체들이 왼쪽으로 움직일 것이라는 것을 이미 알고 있습니다. 이것은 우리에게 움직임에 대한 추측에 대한 직관을 줍니다. 또한, 멀리 떨어진 거리의 물체에 대해서는 천천히 움직이는 반면 주변의 물체는 빠르게 움직이며 카메라가 움직이면 이미지의 물체의 움직임은 시차(disparity)를 형성합니다. 바로 이 시차를 통해 우리는 어느 물체가 멀리 떨어져 있고 어느 물체가 가까운지를 결정할 수 있습니다. 그러나 물체가 멀리 떨어져 있다는 것을 알고 있더라도 여전히 여기에는 상대적인 성질이 있습니다. 예를 들어, 우리가 영화를 볼 때 스크린의 장면에서 어떤 장면이 다른 장면보다 큰지 알 수 있지만 이러한 장면의 실제 크기를 파악할 수는 없습니다.  
단안 카메라가 보는 이미지들에 대한 오브젝트들은 이미지마다 기본적으로 같습니다. 이것은 단안 SLAM에 의해 추정된 궤도와 맵이 실제 궤도와 맵과 다를 것임을 보여줍니다. 이것을 Factor, Scale이라고도합니다. 단안 SLAM을 적용할 때  이미지가 실제 오브젝트의 크기를 판별 할 수 없기 때문에, 그것은 또한 불확실한 크기로 전해져  깊이를 계산하기 위해, 그리고 진정한 스케일을 확인할 수없는 문제가 많이 발생합니다 . 이 문제의 근본 원인은 깊이가 단일 이미지에서 결정될 수 없다는 것에 있습니다. 그래서, 이것을 얻기 위해서 깊이 정보를 제공할 수 있는, 사람들은 양안 및 깊이 카메라를 사용하기 시작했습니다.  

**양안 카메라와 깊이 카메라**

양안 카메라와 깊이 카메라를 사용하는 목적은 어떤 수단을 통해 물체와 우리 사이의 거리를 측정하고 단안 카메라가 거리를 알 수 없다는 단점을 극복하는 것입니다. 거리 정보를 알 수 있다면 장면의 3차원 구조를 단일 이미지에서 복구할 수 있으므로 앞절에서 언급한 크기의 불확실성에 대한 부분을 고려하지 않아도 됩니다. 이 두 개 카메라 모두 거리를 측정하기 위한 도구이지만, 이것을 측정하기 위한 양안 카메라와 깊이 카메라의 원리는 동일하지 않습니다. 양안 카메라는 두 개의 단안 카메라로 구성되어 있으며, 이 두 카메라 간의 거리는 직접 지정할 수 있습니다. 이 말은 우리가 카메라 간 거리(Baseline)를 알고 있다는 말과 동일합니다. 우리는 이 베이스라인(Baseline)을 사용하여 인간의 눈과 마찬가지로 각 픽셀의 공간적 위치를 추정하도록 합니다. 인간은 왼쪽 눈과 오른쪽 눈 이미지의 차이로 물체의 거리를 판단 할 수 있으며 컴퓨터에서도 이러한 원리를 적용하는 것이  가능합니다 (그림 2-5 참조).

![stereo](/resources/ch2/stereo.jpg)  
그림 2-5 양안 카메라 데이터 : 왼쪽 눈 이미지, 오른쪽 눈 이미지. 왼쪽 눈과 오른쪽 눈의 차이를 사용하여 장면의 물체와 카메라 간의 거리를 결정할 수 있습니다.  

컴퓨터의 양안 카메라는 각 픽셀의 깊이를 추정하기 위해 많은 계산 능력이 필요합니다. 이것은 인간에 비해 매우 다르다고 볼 수 있습니다. 양안 카메라는 베이스라인과 관련된 깊이 범위를 측정합니다. 베이스 라인의 거리가 멀수록 먼 거리를 측정 할 수 있기 때문에 자율차의 스테레오 카메라는 일반적으로 서로의 간격이 큽니다. 양안 카메라의 거리 추정은 왼쪽 카메라와 오른쪽 카메라의 이미지를 비교하여 얻어지며, 다른 감지 장치에 의존하지 않으므로 실내외에서 적용 할 수 있습니다. 양안 또는 멀티 카메라의 단점은 구성 및 보정이 복잡하다는 것입니다. 깊이 경로 및 정확도는 이중 목적 베이스라인 및 해상도에 의해 제한되며, 시차 계산은 GPU 및 FPGA 장치에 의해 가속화 되어야합니다. 또한 실시간으로 전체 이미지의 거리 정보를 출력하기 위해 기존 조건 하에서 관측되는 빛의 양에 대한 고려가 여기서의 주된 문제 중 하나입니다.  
깊이 카메라는 2010년경 등장하기 시작한 카메라입니다. 가장 큰 특징은 적외선 구조광 방식 또는 레이저 광선의 비행 시간(Time-of-Flight)을 측정해 깊이 값을 예측한다는 것입니다. ToF카메라의 원리는 레이저 센서와 마찬가지로 물체에 적극적으로 빛을 발사하고 반사된 빛을 받아서 물체와 카메라 사이의 거리를 측정합니다. 이 부분은 양안 카메라와 같은 소프트웨어 계산으로는 해결되지 않지만 물리적 측정 수단을 통해 해결되므로 양안 카메라와 비교하여 큰 계산을 줄일 수 있습니다 (그림 2-6 참조). 현재 일반적으로 사용되는 RGB-D 카메라에는 Kinect / Kinect V2, Xtion Pro Live, RealSense 등이 있습니다. 그러나 대부분의 RGB-D 카메라는 좁은 범위, 높은 노이즈, 작은 시야, 햇빛에 쉽게 노출,  투과 물질 측정 불가 등의 많은 문제점을 품고 있습니다. SLAM에서는  주로 실내 응용에서 사용됩니다.  

![rgbd](/resources/ch2/rgbd.jpg)  
그림 2-6 RGB-D 데이터 : 깊이 카메라는 이미지와 객체의 거리를 직접 측정하여 3차원 구조를 복원 할 수 있습니다.  

우리는 몇 가지 일반적인 카메라에 대해 논의했습니다. 이를 바탕으로  위의 지침을 통해 독자들이 직관적인 이해가 가능하다고 생각됩니다. 이제 장면을 통해 카메라를 움직이는 과정을 상상해보십시오. 우리는 끊임없이 변화하는 일련의 이미지를 얻을 것입니다. 시각적 SLAM의 목표는 이러한 이미지를 사용하여 위치 지정 및 맵 구성을 수행하는 것입니다. 이 문제는 우리가 생각하는 것처럼 간단하지 않습니다. 데이터를 잃어 버리면 위치 정보와 지도 정보를 지속적으로 출력 할 수 없습니다. SLAM은 정교한 알고리즘 프레임 워크를 필요로 하며, 연구자들은 오랜 시간 동안 이 주제에 대해 활발히 연구하였으며, 이러한 연구 끝에 기존 프레임 워크가 완성 될 수 있었습니다.  

# 2.2. 고전적인 시각적 SLAM 프레임워크  

그림 2-7에서 볼 수 있듯이 고전적인 시각적 SLAM 프레임 워크를 살펴 보겠습니다. 이것은 시각적 SLAM이 구성하는 모듈입니다.  

![workflow](/resources/ch2/workflow.jpg)  
그림 2-7 전체 시각적 SLAM 흐름도. 전체 시각적 SLAM 프로세스에는 다음 단계가 포함됩니다.  

  1. 센서 정보를 읽습니다. 시각적 SLAM에서는 주로 카메라 이미지 정보의 읽기 및 전처리 단계를 말합니다. 이 단계에서는 휠 및 관성 센서와 같은 정보를 읽고 동기화 할 수 있습니다.  
  2. Visual odometry (VO). Visual odometry는 인접한 이미지 사이의 카메라 동작과 로컬 맵의 모양을 추정하는 것입니다. VO는 프론트 엔드 라고도합니다.  
  3. Backend optimization: 백엔드는 위 그림에서 loop closure에 대한 정보는 물론 시각적 인 주행 거리계로 측정 한 카메라 포즈를 다른 시간에 받아들이고 일관된 궤도와 맵을 얻을 수 있도록 최적화합니다. VO에 연결되어 있기 때문에 Backend라고도합니다.  
  4. Loop Closure는 로봇이 과거에 도달했던 위치에 도달했는지 확인하는 역할을 수행합니다. loop closure가 감지되면 처리를 위해 백엔드에 정보를 제공합니다.   
  5.맵핑. 추정 된 궤도를 기반으로 현재 위치에 해당하는 지도를 만듭니다.  

고전적인 시각적 SLAM 프레임 워크는 10 년 이상 연구되어온 결과입니다. 프레임 워크 자체와 이 프레임 워크에 포함 된 알고리즘은 기본적으로 기본 틀이 있으며 많은 라이브러리에서 이미 사용할 수 있습니다. 이 알고리즘을 사용하여 정상적인 작업 환경에서 실시간으로 위치를 지정하고 매핑하는 시각적 SLAM 시스템을 구축 할 수 있습니다. 따라서, 우리는 작업 환경이 정적,  조명 변화 등에 국한되지 않는 경우, 전체 알고리즘이 동작할 때 인간의 간섭이 없다고 할 수 있다면, 이 SLAM 시스템은 꽤 성숙하다고 할 수 있을  것입니다. 독자는 위 모듈의 개념을 이해하지 못할 수도 있기 때문에, 각 모듈의 특정 작업을 자세히 설명 해 드리도록 하겠습니다. 그러나 그것이 어떻게 작동하는지에 대한 정확한 이해는 수학적 지식을 필요로하는데, 우리는 책의 두 번째 부분에 넣을 것입니다. 현 챕터에서는 독자는, 각 모듈에 대해 직관적이고 정성적으로 이해할 필요가 있습니다.  

# 2.2.1 Visual Odometry  

Visual odometry는 인접한 이미지 사이의 카메라 움직임과 관련이 있습니다. 가장 간단한 경우는 두 이미지 간의 모션 관계입니다. 예를 들어, 그림 2-8을 보면, 오른쪽 이미지는 왼쪽 이미지를 특정 각도만큼 왼쪽으로 회전시킨 결과여야만 한다는 것을 자연스럽게 반영합니다. 그것에 대해 생각해 봅시다 : "왼쪽으로 회전"하는 것을 어떻게 알수 있을까요? 인간은 오랫동안 세계를 자신의 눈으로 탐험하고 자신의 위치를 추측하는 데 익숙해져왔지만 이성적인 언어로 이러한 직감을 표현하는 것은 종종 어렵습니다. 그림 2-8을 보면, 카메라가 회전함에 따라 보여지는 장면이 변화하게 됩니다. 이 정보를 사용하여 카메라를 왼쪽으로 회전해야한다고 판단합니다.  

![cameramotion](/resources/ch2/cameramotion.jpg)  
그림 2-8 카메라와 사람의 눈으로 촬영 한 그림의 움직임 방향.  

그러면 조금 더 나아가서 얼마나 많은 회전 각도와 몇 센티미터를 이동했는지 확인할 수 있습니까? 여기에 대한 질문은 실제로 확실한 답을 내리는 것이 어렵습니다. 우리의 직감이 특정한 숫자에 민감하게 반응하지 않기 때문입니다. 그러나 컴퓨터에서는 이 동작 정보를 정확하게 측정해야합니다.  
그래서 우리는 물어야합니다 : 컴퓨터가 이미지를 통해 카메라의 움직임을 어떻게 결정합니까? 또한 이전, 컴퓨터 비전 분야에서 언급한, 인간은 직관적으로 매우 자연스러운 일을 보이지만, 인간 시각중심적인 내용은 컴퓨터를 이해시키기엔  매우 어렵습니다. 이미지는 컴퓨터에서 행렬 값으로 표현됩니다. 이 행렬에 표현 된 것에 대해 컴퓨터의 시각에서는 직관적 개념이 없습니다 (이것은 기계 학습이 이제 해결해야하는 문제입니다). Visual SLAM에서는 카메라 투영 결과, 면의 공간 점 중 어떤 것을 알고 하나의 화소를 알 수 있습니다. 따라서 카메라 동작을 정량적으로 추정하려면 먼저 카메라와 공간 점 사이의 기하학적 관계를 이해해야합니다. 이 기하학적 관계와 VO의 구현을 명확히 하기 위한 배경을 세워야합니다. 여기에서는 먼저 독자가 VO의 직관적인 개념을 갖도록합니다. VO가 인접 프레임 사이의 이미지를 통해 카메라 모션을 추정하고 장면의 공간 구조를 복원 할 수 있어야합니다. 그것은 단지 계산된 동작 시간의 이웃이지만 실제로는 로봇이 주행할 때 "주행"을 호출 한 다음 앞으로 이동하고 과거의 정보가 연결되어 있지 않습니다. 이 시점에서 VO는 짧은 기억성을 가진다고 생각 해 봅시다. 이제 두 이미지 사이의 카메라 움직임을 추정하는 시각적 주행 거리계가 있다고 가정합니다. 위치 문제를 해결하기 위해 함께 로봇의 궤도를 구성하는 시간 사이의 인접한 움직임을 생각하여, 각 순간의 카메라 위치에 기초하여 각 픽셀에 대응하는 공간 지점의 위치를 ​​계산하고, 맵을 얻습니다.  
Visual odometry는 SLAM의 핵심이며, 이를 소개하는 데 많은 시간을 할애 할 것입니다. 그러나 visual odometry를 통해서만 궤도를 추정하면 필연적으로 에러가 누적됩니다. 이것은 visual odometry (가장 간단한 경우)가 두 이미지 사이의 동작만 추정하기 때문입니다. 각 추정치에는 일정한 오류가 있으며 odometry가 작동하기 때문에 이전 순간의 오류가 다음 순간으로 전달되어 예상 된 궤도가 일정 기간 후에 정확하지 않게됩니다 (그림 2.9참조). 예를 들어, 로봇은 먼저 왼쪽으로 90 ° 회전하고 오른쪽으로 90 ° 회전합니다. . 오류로 인해 우리는 처음 90 °를 89 °로 추정했다고 생각 해 봅시다. 그런 다음 우회전 후 로봇의 예상 위치가 원래 위치로 돌아오지 않았음을 알 수 있습니다. 설상가상으로, 후속 추정치가 정확하더라도, 실제 값과 비교하면 -1 ° 오차가 나타납니다.  

![loopclosure](/resources/ch2/loopclosure.jpg)  
그림 2-9 누적 오류 및 loop closing 감지 수정 결과  

이것은 drift problem이라고합니다. 이 때문에 일관된 전체 지도를 세울 수 없게 됩니다. 위 그림을 통해 최초의 직선이 기울어졌음을 알게 될 것입니다. 직각이 되어야 하는 부분이 어색하게 되었습니다. 드리프트 문제를 해결하려면 Backend optimization 및 Loop closure detection의 두 가지 기술이 필요합니다. Loop closure detection는 "로봇의 현재 위치가 이전에 방문 했던 곳인가 "를 감지하고 Backend optimization는 이 정보를 기반으로 전체 트랙의 모양을 수정합니다.  

# 2.2.2 Backend optimization  

일반적으로 Backend optimization에서는 주로 SLAM 중 노이즈를 처리하는 문제를 수행합니다. 모든 데이터가 정확하기를 바랄지라도 실제 센서에는 정밀한 센서에도 약간의 노이즈가 존재합니다. 값이 저렴한 센서는 측정 오류가 큰 대신에 비용이 적게 듭니다. 또한, 일부 센서는 자기장과 온도에 영향을받습니다. 따라서 "이미지에서 카메라 모션을 추정하는 방법"을 해결하는 것 외에도 이 추정치가 얼마나 많은지, 이전 순간부터 다음 순간으로 전달되는 방식에 대해 신경을 쓰고 있습니다. Backend optimization에서 고려해야 할 문제는 이러한 잡음이 많은 데이터로부터 전체 시스템의 상태를 추정하는 방법과 상태 추정치가 얼마나 불확실한 지 - 이것이 최대 사후 확률 추정이라고합니다(Maximum a Priori, MAP).  반대로 Visual odometry 부분을 "프런트 엔드"라고도 합니다. SLAM 프레임 워크에서 프런트 엔드는 백엔드에 최적화 할 데이터와 데이터의 초기 값을 제공합니다. 백엔드는 전반적인 최적화 프로세스를 담당하며 데이터 출처에 신경을 쓰는 것 보단 데이터 자체를 다룹니다. 카메라 SLAM 프런트 엔드에서는 컴퓨터 등의 화상의 특징 추출 및 정합 필터링 메인 비선형 최적화 알고리즘이 이용됩니다. 역사적으로 우리가 지금 Backend optimization라고 부르는 것은 오래 전부터 "SLAM 연구"로 불려 왔습니다. 초기 SLAM 문제는 상태 추정 문제였습니다. 이것은 바로 Backend optimization가 해결해야만했던 문제였습니다. 추정 상태 공간의 불확실성을 다루는 논문의 일련의 제안이 있어왔으나 그것은 SLAM 문제의 본질을 반영은 하지만 SLAM 문제를 좀 더 정확하게 풀기 위해서는 위치 추정과 매핑의 불확실성을 표현하기위한 상태 추정 이론이 필요했습니다. 그런 다음 필터 또는 비선형 최적화를 사용하여 상태의 평균과 불확실성 (분산)을 추정해야 합니다. 상태 추정과 비선형 최적화의 세부 사항은 강의 6, 10 및 11에서 소개됩니다. 당분간 이 원칙에 대한 설명을 건너 뛰고 계속해서 소개하겠습니다.  

# 2.2.3 Loop closure detection  

Loop Closure Detection으로 알려진 루프 감지는 주로 시간에 따른 위치 추정 문제를 해결하는 것을 목적으로 합니다. 그것을 해결하는 방법은 로봇이 실제 상황에서 일정 기간 운동 한 후 원점으로 돌아가지만 드리프트로 인해 위치 추정치가 원점으로 복귀하지 않는다고 가정하는 경우입니다. 어떻게 해야 할까요? 우리는 로봇이 "원점 복귀"또는 "원점"을 식별할 수있는 방법이 있다면 위치 추정치를 "끌어 와서" 에러를 제거 할 수 있다고 생각합니다. 이를 Loop closure detection라고합니다. Loop closure detection는 "위치 지정"및 "맵핑"과 밀접하게 관련됩니다. 사실, 우리는 지도의 주된 의미는 로봇이 어디에 있었는지 알려주는 것이라고 믿습니다. Loop closure detection을 구현하려면 로봇에게 도달 한 장면을 인식 할 수 있어야 합니다. 그것을 성취 할 수 있는 많은 방법이 있습니다. 예를 들어, 위에서 언급 한 것처럼 로봇 아래에 마커 (예 : QR 코드 이미지)를 설정할 수 있습니다. 이 표지판을 보자 마자 당신이 원점으로 돌아 간다는 것을 알게됩니다. 그러나 마커는 본질적으로 환경의 센서이므로 응용 프로그램 환경을 제한합니다 (QR 코드를 붙여 넣을 수없는 경우에는 어떻게해야합니까?). 우리는 또한 로봇에 장착된 센서, 즉 이미지 자체를 사용하여이 작업을 수행하기를 바랍니다. 예를 들어, 이미지 간의 유사성을 판단하여 Loop closure detection을 완료 할 수 있습니다. 이것은 사람들과 비슷합니다. 우리가 비슷한 그림 두 개를 볼 때, 그들이 같은 장소에서 왔음을 쉽게 알 수 있습니다. '루프백 테스트가 성공하면 누적 오류를 크게 줄일 수 있습니다. 따라서 시각적 Loop closure detection는 기본적으로 이미지 데이터의 유사도를 계산하는 알고리즘입니다. 그렇지만 이미지 정보는 영상 내에 아주 풍부하기 때문에 루프백을 정확하게 감지하기란 쉽지가 않습니다. 그렇지만 루프백을 감지 한다면 이후 Backend optimization 알고리즘에 "A와 B는 같은 지점입니다."라는 메시지를 알려줍니다. 백엔드에서는 이 새로운 정보에 기반한 Loop closure detection 결과와 일치하도록 궤도 및 맵을 조정합니다. 이런 방식으로, Loop closure detection가 충분하고 정확하다면 우리는 누적 오류를 제거하고 전역적으로 일관된 맵을 얻을 수 있습니다.  

# 2.2.4 맵핑(Mapping)  

맵핑은 맵을 빌드하는 과정을 나타냅니다.(그림 2-10 참조) 맵은 환경에 대한 설명이 가능하지만, 환경에 대한 설명이 항상 고정 되어 있지 않으며 SLAM의 응용 프로그램에 따라 다르게 구현됩니다.  

![map](/resources/ch2/map.jpg)  
그림 2-10. 다양한 지도 표현법  

무인 청소 로봇의 경우, 주로 낮은 평면에서 움직이는 이 로봇은 통과 할 수있는 2 차원 지도만 있으면 되고 장애물이있는 경우에는 특정 범위 내에서 탐색하면 충분합니다. 그리고 카메라의 경우 6개 자유도가 있으며 3차원 지도를 필요로 합니다. 때때로 우리는 공간적 집합뿐만 아니라 질감이있는 삼각형 패치도 재구성 하기를 원합니다. 다른 때에는 지도가 보이는 방식에 신경 쓰지 않고 "A에서 B까지 통과 할 수 있고 B에서 C까지 통과 할 수 없습니다"와 같은 것을 알아야합니다. 때로는 지도가 필요하지 않거나 다른 사람들이 지도를 제공 할 수도 있습니다. 예를 들어 움직이는 자동차는 종종 그려진 지역 지도(local map)의 지도를 얻을 수 있습니다. 지도에는 아이디어와 필요에 의한 요구가 많습니다. 따라서 앞서 언급 한 Visual odometry, Loop closure detection 및 Backend optimization와 비교할 때, 매핑을 위한 고정된 형식과 알고리즘은 없습니다. 공간 점 들의 집합은 지도라고 할 수 있습니다. 아름다운 3D 모델은 지도, 도시, 마을, 철도, 강 또는 지도를 표시하는 그림이기도 합니다. 맵의 형식은 SLAM의 응용 프로그램에 따라 다릅니다. 일반적으로 이 지도를 메트릭 맵과 토폴로지 맵으로 나눌 수 있습니다.

**Metric map**

메트릭 맵은 일반적으로 sparse (희소성) 및 Dense (밀도)로 분류 된 맵에서 오브젝트의 위치 관계를 정확하게 표현하는 데 중점을 둡니다. Sparse map은 어느 정도 추상화되어 있으며 모든 객체를 표현할 필요가 없습니다. 예를 들어 Landmark라고하는 대표적인 것을 선택하면 sparse map은 도로 표지판이 아닌 도로 표지판의 지도입니다. 대조적으로 dense map은 보는 모든 것을 모델링하는 데 중점을 둡니다. 위치 확인을 위해서는 sparse map으로 충분합니다. 네비게이션에 사용될 때에는 밀도가 높은지도가 필요할 때가 많습니다 (그렇지 않으면 두 개의 도로 표지판 사이에 벽을 치면 어떨까요?). dense map은 일반적으로 특정 해상도의 여러 작은 블록으로 구성됩니다. 2 차원 메트릭 맵에는 많은 작은 그리드가 있고 3 차원 메트릭 맵에는 많은 작은 사각형 (Voxel)이 있습니다. 일반적으로 작은 블록에는 셀에 객체가 있는지 여부를 나타 내기 위해 점유, 유휴 및 알 수없는 세 가지 상태가 포함됩니다. 공간 위치를 쿼리 할 때, 지도는 위치가 통과 할 수 있는지 여부에 대한 정보를 제공 할 수 있습니다. 이러한 지도는 로봇 연구가가 가치를 지닌 다양한 탐색 알고리즘에 사용할 수 있습니다. 그러나 이 맵은 많은 그리드 포인트의 상태를 저장해야하므로 많은 저장 공간을 필요로하며, 대부분의 경우 맵의 많은 세부 사항은 쓸모가 없습니다. 반면, 대규모 메트릭 맵에는 때때로 일관성 문제가 있습니다.

**Topological map**

토폴로지 맵은 맵 요소의 관계를 메트릭 맵의 정확도와 비교하여 강조합니다. 토폴로지 맵은 노드와 에지로 구성된 그래프로 노드 간의 연결 만 고려합니다. 예를 들어 A 지점에서 B 지점에 도달하는 방법에 관계없이 A 지점과 B 지점이 연결되어 있습니다. 정확한 위치에 대한지도의 필요성을 완화하고, 지도의 세부 사항을 제거하여보다 컴팩트 한 표현을 제공합니다. 그러나 토폴로지 맵은 복잡한 구조의 맵을 표현하는 데 적합하지 않습니다. 노드와 에지를 구성하기 위해 맵을 분할하는 방법과 탐색 및 경로 계획에 토폴로지 맵을 사용하는 방법은 여전히 연구해야 할 문제입니다.  

# 2.3 SLAM 문제의 수학적 표현  

이전 절에 소개된 내용으로부터 독자는 SLAM의 각 모듈의 구성과 주요 기능을 직관적으로 이해했을 것입니다. 그러나 직관적인 측면에만 의존한다고해서 우리가 실행할 수 있는 프로그램을 작성하는 데 도움이 되지는 않습니다. 합리성을 위해서는 SLAM 프로세스를 수학 언어로 설명해야합니다. 몇 가지 변수와 수식을 사용하겠습니다만 충분히 명확하게 유지하도록 노력하겠습니다.  
어떤 환경에서 자율적으로 이동하기 위해 로봇가 특정 센서를 들고 있다고 가정합니다. 어떻게 이것을 수학 언어로 설명할까요? 첫째, 카메라는 보통 특정 시간에 데이터를 수집하기 때문에 이러한 순간의 위치와  지도만 신경씁니다. 이것은 연속 시간 운동을 이산적인 타임프레임 ![formula01](/resources/ch2/formula01.gif)로 바꿉니다. 이 순간에 변수 x는 로봇 자체의 위치를 나타내는 데 사용됩니다. 그러면 각 순간의 위치는 작은 로봇의 궤도를 구성하는 ![formula02](/resources/ch2/formula02.gif)로 기록됩니다. 지도의 관점에서, 우리는 다수의 표식 (랜드 마크)으로 구성된지도를 설정하고, 매 순간마다 센서는 표출의 일부를 측정하여 관측치를 얻습니다. ![formula03](/resources/ch2/formula03.gif) 으로 표현되는 총 N 개의 랜드 마크를 설정합시다.  
이 설정에서 작은 로봇에 환경과 상호작용하여 움직이는 센서에 대해 두 가지 개념으로 설명합니다.  

  1. 운동 모델 (motion model)
  로봇의 위치 x가 k-1에서 k로 어떻게 변하는 지 고려해야합니다.  
  2. 관찰 모델 (sensor model)
  로봇은 시간 k의 위치 ![formula04](/resources/ch2/formula04.gif)에서 특정 랜드 마크 ![formula05](/resources/ch2/formula05.gif)를 검출했다고 가정해 봅시다.이 문제가 어떻게 수학언어로 기술되는지 고려해야합니다.  

먼저 운동 모델을 예로 살펴 봅시다. 일반적으로 로봇은 휠 인코더나 관성 센서와 같이 자체 동작을 측정하는 센서를 가지고 있습니다. 이 센서는 동작에 대한 판독 값을 측정 할 수 있지만 위치의 차이뿐만 아니라 가속도, 각속도 및 기타 정보도 다릅니다. 그러나 어떤 센서인지에 관계없이 다음과 같은 일반적인 추상 수학 모델을 사용할 수 있습니다.   

![formula06](/resources/ch2/formula06.gif)

여기에서 ![formula07](/resources/ch2/formula07.gif)는 모션 센서 입력이고 ![formula08](/resources/ch2/formula08.gif)는 노이즈입니다. ![formula09](/resources/ch2/formula09.gif)는 motion model을 나타내는 함수입니다. 이를 통해 전체 기능이 모든 모션 센서를 참조 할 수 있으며 특정 센서에 국한되지 않고 일반적인 방정식이 됩니다. 우리는 그것을 운동 방정식이라고 부릅니다.    
운동 방정식에 해당하는 관찰 방정식도 있습니다. 관측 방정식은 로봇이 ![formula10](/resources/ch2/formula10.gif) 위치에서 어떤 랜드 마크 점 ![formula11](/resources/ch2/formula11.gif)를 볼 때 관측 데이터 ![formula12](/resources/ch2/formula12.gif)가 생성된다는 것을 설명합니다. 다시,이 관계를 설명하기 위해 추상 함수 h를 사용합니다.  

![formula13](/resources/ch2/formula13.gif)  

여기서 ![formula14](/resources/ch2/formula14.gif)는 이 관찰에서의 잡음이다. 관측에 사용 된 센서는 형태가 더 많으므로 관측 자료 z와 관측 방정식 h는 여러 가지 형태로 존재합니다.  
동시에 x, y, z는 무엇입니까? 사실, 로봇의 실제 동작과 센서의 유형에 따라 몇 가지 매개 변수화 방법이 있습니다. 이 매개 변수화란 예를 들어, 작은 로봇이 평면에서 움직인다고 가정하면 자세에 대한 설명은 두 위치와 모서리, 즉 ![formula15](/resources/ch2/formula15.gif)에 의해 설명됩니다. 동시에 모션 센서는 매 시간 간격 ![formula16](/resources/ch2/formula16.gif)에서 로봇의 위치와 회전 각도의 변화를 측정 할 수 있습니다. 그러면 운동 방정식은 다음과 같이 구현할 수 있습니다.  

![formula17](/resources/ch2/formula17.gif)
